# Public: Describes remote Redshift data sources that use S3 to unload data.
class RedshiftS3DataSource < DataSource

  # Gets the parent DataSource object.
  attr_reader :data_source


  # Public: Initializes a Redshift data source.  Typically, a
  # Redshift data source SQL is just a 'select * from schema.table'
  # type query.  The schema and table parameters here are not
  # required if the SQL does not include references to their class
  # names, but they do form part of the path on S3, so it is wise to
  # make these unique for each data source (see doc in each_chunk
  # method for examples).
  #
  # data_source     - The DataSource object that contains this RedshiftS3DataSource instance
  # redshift_schema - The SQL query string to be performed and
  #                   unloaded to S3. The query should be a simple
  #                   single SQL statement (no semicolons). Note that
  #                   @redshift_schema will be replaced by the values
  #                   of @redshift_schema.  This can be useful for
  #                   defining generic SQL queries (default: "select *
  #                   from @redshift_schema.mytable").
  def initialize(data_source, redshift_schema: nil)
    @data_source = data_source
    @redshift_schema = redshift_schema

    @pg_conn = nil
    @s3_bucket_name = Settings.data_sources.redshift_s3.s3_staging_path[/s3:\/\/([\w-]+)\//,1]
    @s3_bucket = AWS::S3.new.buckets[@s3_bucket_name]
    @s3_table_folder = "#{Settings.data_sources.redshift_s3.s3_staging_path[/s3:\/\/[\w-]+\/(.*)/,1]}/#{@redshift_schema || "NOSCHEMA"}-#{@data_source.name}-#{Time.now.strftime('%Y%m%d-%H%M%S%z')}-#{SecureRandom.hex(2)}"

    @pg_conn = nil
  end


  # Public: Reads the S3 files generated by the Redshift query.
  #
  # block - A block containing commands used to manipulate the yielded data.
  #
  # Yields a chunk of data returned from Redshift via S3.
  def read(&block)
    initialize_connection
    exec_query

    yield header

    # There were some issues with streaming data directly from S3 to
    # Birst.  Independently, they both work just fine, but some sort
    # of interference was happening while rapidly switching between
    # S3 read statements and Birst SOAP requests.  The issue would
    # quite frequently result in network timeout errors.  The
    # solution seems to be to bring the whole file to the local
    # filesystem and then upload to Birst.  I don't like this; it
    # could of course become problematic for really large files.

    Dir.mktmpdir(File.join("Amylase-#{UUID.new.generate}"), Dir.tmpdir) do |tmpdir|

      redshift_filenames = []
      @s3_bucket.objects.with_prefix(@s3_table_folder).each do |obj|
        redshift_filenames << File.join(tmpdir,obj.key.split('/').last)

        File.open(redshift_filenames.last, 'wb') do |file|
          obj.read do |chunk|
            file.write(chunk)
          end
        end
      end

      redshift_filenames.each do |filename|
        gz = Zlib::GzipReader.new(File.new(filename))

        gz.each_line do |gz_line|
          yield gz_line
        end

        gz.close
      end
    end

    clear_staging_s3_folder
  end


  private

    # Private: Initailizes the postgres connection to Redshift.
    def initialize_connection
      @pg_conn = PG.connect(
        host:     Settings.data_sources.redshift_s3.host,
        dbname:   Settings.data_sources.redshift_s3.dbname,
        port:     Settings.data_sources.redshift_s3.port,
        user:     Settings.data_sources.redshift_s3.user,
        password: Envcrypt::Envcrypter.new.decrypt(Settings.data_sources.redshift_s3.password),
        sslmode:  Settings.data_sources.redshift_s3.sslmode
       )
    end

    # Private: Deletes any data that exists in the S3 path where the results
    # will be UNLOADed.
    def clear_staging_s3_folder
      if [nil, "", "/"].include? @s3_table_folder
        raise "s3_table_folder malformed - cowardly refusing to delete all data in s3 bucket #{@bucket_name}"
      end

      @s3_bucket.objects.with_prefix(@s3_table_folder).each { |obj| obj.delete }
    end


    # Private: Builds the full UNLOAD query that is sent to Redshift.
    #
    # Returns a string with the full Redshift UNLOAD query.
    def query
      <<-EOT.unindent
        UNLOAD('#{parse_sql.gsub("'","\\\\'")}')
        TO 's3://#{@s3_bucket_name}/#{@s3_table_folder}/#{@data_source.name}-'
        CREDENTIALS 'aws_access_key_id=#{Settings.aws.access_key_id};aws_secret_access_key=#{Settings.aws.secret_key}'
        ESCAPE
        GZIP
      EOT
    end

    # Private: Replaces @redshift_schema in the SQL string.
    def parse_sql
      @data_source.redshift_sql.gsub(/(@redshift_schema)/, "@redshift_schema" => @redshift_schema)
    end

    # Private: Run the same query with limit 0 to get the header string.
    #
    # Returns a pipe-delimited list of field names.
    def header
      result = @pg_conn.exec("#{parse_sql} limit 0")
      result.fields.join('|') + "\n"
    end

    # Private: Executes the Redshift query.
    def exec_query
      @pg_conn.exec(query)
    end

end
